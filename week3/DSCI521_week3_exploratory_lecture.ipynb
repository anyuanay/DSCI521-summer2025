{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/DSCI521-summer2025/blob/main/week3/DSCI521_week3_exploratory_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcHTbWx79NAd"
      },
      "source": [
        "# DSCI 521: Methods for analysis and interpretation <br> Chapter 3: Exploratory data analysis and visualization\n",
        "\n",
        "## 3.0 What is Exploratory Data Analysis (EDA)?\n",
        "\n",
        "[Thanks Wikipedia!](https://en.wikipedia.org/wiki/Exploratory_data_analysis)\n",
        "\n",
        "> In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n",
        "\n",
        "In a very real way, EDA is at the heart of data science. This is primarily due to the flipped nature of data science's scientific method in which data often exist before an experiment with which said data is well defined. However, given the non-linear nature of our scientific method it's arguable that there are two primary goals/outcomes for EDA:\n",
        "\n",
        "1. Evaluating the integrity of a data set to guide data acquisition and pre-processing.\n",
        "    + Oftentimes, EDA will result in the reformulation of data collection methods, or the decision to collect different data.\n",
        "2. Forming expectations and an intution for the usefulness and potential application of a dataset.\n",
        "    + Oftentimes, EDA will result in the generation of a hypothesis, and plan for a model or experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srQehnsd9NAe"
      },
      "source": [
        "### 3.0.1 What Methods are Used for EDA?\n",
        "In general, EDA should provide you with a broad sense of what your data set contains. This sort of a big picture is ordinarily obtained through one of:\n",
        "+ summarization: through measures of commonality, centrality, variation, association, and regularity; or\n",
        "+ visualization: through succicnt and intuitive representations that allow you to see all of the data.\n",
        "\n",
        "We'll explore methods for each of these in turn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLOlWKPh9NAf"
      },
      "source": [
        "### 3.0.2 A Caveat: Communication\n",
        "While EDA might be focused on internal matters and progressing a project forward, its tools often serve an addition purpose for data science: __communication__. Summarization and visualization not only allow us to form an intuition about what our data can do, they allow us to succicntly describe and/or exhibit specific knowledge about the project we are working on to other who might not have the same experience with it. So, as with code documentation it's always important to think about how summarizations and visualization (especially) can be made pretty, precise, and digestible for outside audiences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JKeeYD99NAf"
      },
      "source": [
        "## 3.1 Summarization Methods\n",
        "### 3.1.1 Commonality and Sorting\n",
        "One of the most straightforward ways to get in idea for the big features at play in a data set is to ask the question\n",
        "\n",
        "> \"what occurs most frequently?\"\n",
        "\n",
        "We'll refer to this as commonality, with the most most common thing being the _mode_. Commonality can come up in a few different ways (see histograms below), but the simplest scenario to think of it is probably in the case of categorical or discrete variables, i.e., in the case of occurances that you can count. Reverse sorting (high to low), is also referred to as _ranking_.\n",
        "\n",
        "#### 3.1.1.1 Using the `sorted()` function\n",
        "The canonical way to sort an iterable object in Python is with the `sorted()` function. Let see what `sorted()` can do:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2H8VK6r9NAf",
        "outputId": "4d2b916e-661f-4b20-9a4a-d865d9192a6e"
      },
      "source": [
        "```python\n",
        "## make some data to sort\n",
        "list_of_numbers = [8, 6, 7, 5, 3, 0, 9]\n",
        "\n",
        "## by default, sorted is ascending\n",
        "print(sorted(list_of_numbers), '\\n')\n",
        "\n",
        "## reverse=True make the sort ascend\n",
        "print(sorted(list_of_numbers, reverse = True), '\\n')\n",
        "\n",
        "## you can also sort string alphabetically\n",
        "a_string = \"If you sort this string it won't be readable!\"\n",
        "print(sorted(a_string), '\\n')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzMLRQAD9NAf"
      },
      "source": [
        "A list of tuples automatically sorts by first elements; ties are then resolved by subsequent elements, i.e., 'columns':"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b52Cz7mJ9NAf",
        "outputId": "04893ca8-b4e7-4526-b103-f685d93da875"
      },
      "source": [
        "```python\n",
        "## make some data to sort\n",
        "list_of_tuples = [\n",
        "    (8, \"g\"), (6, \"a\"), (7, \"f\"), (5, \"d\"),\n",
        "    (3, \"k\"), (0, \"t\"), (9, \"x\"), (8,  \"f\")\n",
        "]\n",
        "\n",
        "print(sorted(list_of_tuples))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Ukvosr9NAf"
      },
      "source": [
        "Perhaps most importantly, you can use lambda functions to sort by a transformation, for example the second 'column':"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufymKghS9NAf",
        "outputId": "721bcd5f-37cf-4fca-927a-a805b4c7a8a9"
      },
      "source": [
        "```python\n",
        "## sort by the second element of a tuple with a lambda function\n",
        "print(sorted(list_of_tuples, key = lambda x: x[1]))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhFCpijo9NAf"
      },
      "source": [
        "#### 3.1.1.2 Exercise: modifying a sort order\n",
        "Use the `key` argument and a lambda function to sort the `list_of_tuples` object primarily by the second (string) column, and secondarily by the first (integer) column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbj7Z0NE9NAg"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSpTIE7t9NAg"
      },
      "source": [
        "#### 3.1.1.3 Finding the most common word\n",
        "This can be a simple first step to seeing what a document is about, and also usually figures into the creating of a word cloud. Here, to make things simple we'll use the `Counter()` object type to count up words and do a `.most_common()` sort method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJkWfhi9NAg",
        "outputId": "38428320-eddf-4093-8c5a-6ebd6129e11e"
      },
      "source": [
        "```python\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "## some data\n",
        "above_paragraph = \"We'll refer to this as commonality. Commonality can come up in a few different ways (see histograms, below), but the simplest scenario to think of it is probably in the case of categorical or discrete variables, i.e., in the case of occurances that you can count. In fact, we've already done this for homework the case of the Indego bike-share data, figuring out which bikes were rented out the greatest number of times. This is also referred to as _ranking_. To run with an example, let's count the words in this paragraph and figure out which are the most common with the `sorted()` function.\"\n",
        "\n",
        "## split the data into a list of \"words\"\n",
        "words = re.split(\" \", above_paragraph)\n",
        "\n",
        "## initialize a Counter()\n",
        "word_counts = Counter()\n",
        "for word in words:\n",
        "    word_counts[word] += 1\n",
        "\n",
        "## .most_common() is a built-in method of Counter()\n",
        "## that provides a tuple of the (object, count) by commonality\n",
        "for word, count in word_counts.most_common()[0:10]:\n",
        "    print(word, count)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm0NiYWf9NAg"
      },
      "source": [
        "### 3.1.2 Centrality\n",
        "Centrality has a similar feel to commonality as a summarization method, but has more to do with numeric data. There are actually a number of ways to measure the centrality of a so-called _distribution_ of numbers.\n",
        "\n",
        "#### 3.1.2.1 Averages\n",
        "The way you're probably most used to finding the central tendency of a collection of $n$ numbers, $x_1, \\cdots, x_n$ is called the arithmetic mean:\n",
        "$$\\overline{x}_a = \\frac{1}{n}\\sum_{i=1}^nx_i$$\n",
        "\n",
        "As it turns out, the arithmetic mean is not the only mean out there. There are actually three Pythagorean means:\n",
        "\n",
        "<img src=\"https://github.com/anyuanay/DSCI521-summer2025/blob/main/week3/means.png?raw=true\" />\n",
        "\n",
        "These other two means are generally used less frequently than the arithmetic \"average\", but have their applications. The geometric mean:\n",
        "\n",
        "$$\n",
        "\\overline{x}_g = \\sqrt[n] {\\prod_{i=1}^nx_i}\n",
        "$$\n",
        "\n",
        "is most commonly used for quantities that might get multiplied together, like probabilities, but can't accept negative values!\n",
        "\n",
        "Finally, there's the harmonic mean:\n",
        "$$\\overline{x}_h = \\frac{n}{\\sum_{i=1}^n\\frac{1}{x_i}},$$\n",
        "\n",
        "which is commonly applied to rates, but can't accept non-positive values! For a little more intuition on when to apply the different means, there's a good post about it here:\n",
        "\n",
        "+ https://betterexplained.com/articles/how-to-analyze-data-using-the-average/\n",
        "\n",
        "Working with data on baseball player heights and weights, here's a few ways to compute averages:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69V2hdl9NAg",
        "outputId": "e4fa4525-061b-473d-e852-05a696012737"
      },
      "source": [
        "```python\n",
        "## some modules to help with averages\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import pandas as pd\n",
        "\n",
        "baseball_data = pd.read_csv(\n",
        "    filepath_or_buffer= \"baseball_heightweight.csv\",\n",
        "    sep = \",\",\n",
        "    header = 0\n",
        ")\n",
        "\n",
        "## calculate arithmetic mean with numpy\n",
        "print(np.mean(baseball_data[\"Height\"]), '\\n')\n",
        "\n",
        "## calculate the geometric mean with scipy\n",
        "print(scipy.stats.gmean(baseball_data[\"Height\"]), '\\n')\n",
        "\n",
        "## calculate the harmonic mean with scipy\n",
        "print(scipy.stats.hmean(baseball_data[\"Height\"]), '\\n')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkn4_ucc9NAg"
      },
      "source": [
        "#### 3.1.2.2 Which average actually gets us the 'center'?\n",
        "The answer is 'it depends', both on what the data represent, and on how they are distributed. Notice that when they are defined the above means always stick to the inequality:\n",
        "\n",
        "$$x_h \\leq x_g \\leq x_a$$\n",
        "\n",
        "However, sometimes none of these are really at the center! All of the means have some sensitivity to _outliers_, which are values that are wildly large or small. Well, very often another measure called the _median_ is better.\n",
        "\n",
        "#### 3.1.2.3 Medians\n",
        "A completely different measure of the center of a distribution of numbers is called the _median_, which is the middle number in a sorted list (or average of the two middle, if there are an even number of them). Most importantly, changing the largest number in a data set to something astronomically large won't change the median at all!\n",
        "\n",
        "Believe it or not, this&mdash;because of sorting&mdash;is slightly more complex to compute than any of the means, especially for a large data set. However, we can do it easily in `NumPy`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGYtBAmj9NAg",
        "outputId": "220fc8e4-ead2-44b2-a5f8-e5012d8a3742"
      },
      "source": [
        "```python\n",
        "print(np.median(baseball_data[\"Height\"]))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6FqJVft9NAg"
      },
      "source": [
        "### 3.1.3 Variation\n",
        "Here, instead of a high-level picture of where the middle is, we're interested in knowing how densely packed or spread out data are. As usual, there are a few ways to accomplish this.\n",
        "\n",
        "#### 3.1.3.1 Percentiles\n",
        "As it turns out, medians are just an instance of a more general kind of analysis called _percentiles_. For the median (the 50th percentile), 50% of the falls on either side. This is why it is thought of as a kind of middle. However, the concept generalizes to provide an idea of where, say, the middle 50%, a.k.a the _interquartile range_ (IQR), of your data are located. In particular:\n",
        "\n",
        "+ The $p$th percentile: is the number below which $p$-percent of the data fall\n",
        "\n",
        "To take percentiles we can once again use `NumPy`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3AWgMw9NAg",
        "outputId": "9875abc0-20fd-4924-d3b4-0bdcc562879b"
      },
      "source": [
        "```python\n",
        "## find the 50th percentile (median)\n",
        "print(np.percentile(baseball_data[\"Height\"], 50), '\\n')\n",
        "\n",
        "## find the 25th and 75th percentiles\n",
        "## (first and third quartiles)\n",
        "## This is the middle fifty-percent\n",
        "print(np.percentile(baseball_data[\"Height\"], [25, 75]))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-X6wP5P9NAg"
      },
      "source": [
        "#### 3.1.3.2 Exercise: percentiles\n",
        "Compute all deciles ($10$-increment percentiles) for the baseball player heights. How do these values space apart, are the locations evenly spaced?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ys6n7SJ9NAg"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ETmVX39NAg"
      },
      "source": [
        "#### 3.1.3.3 Variance\n",
        "It should be no surpise that the quantitative concept of 'variance' falls under our discussion of variation! We've talked about averages and measures of centrality, and that kind of thinking can also get you into a measure of spread. The variance is itself an average, but tells you the average squared distance of you data away from its average:\n",
        "$$\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\overline{x})^2$$\n",
        "\n",
        "But wait, why this squared thing? Because the deviations inside of the square could otherwise cancel out! We want to know how far they usually are&mdash;a kind of distance. However this puts the variance out of the units of the original data. So if you want to use means for variance, we use something called the _standard deviation_.\n",
        "\n",
        "The _standard deviation_ is simply the square root of the variance:\n",
        "$$\\sigma = \\sqrt{\\sigma^2}$$\n",
        "\n",
        "Because this takes the square root of a square, it brings the measure back into the units of the original data, allowing for comparison. As it turns out, this brings the concept full circle into another (distance), but first let's look at how we easily compute standard deviation with `NumPy`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6R7N11B9NAg",
        "outputId": "26e4d79f-c133-4d70-9ecf-a64b4223de07"
      },
      "source": [
        "```python\n",
        "## find the standard deviation\n",
        "print(np.std(baseball_data[\"Height\"]))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5kfd0Y19NAg"
      },
      "source": [
        "### 3.1.4 Distance and norms\n",
        "As it turns out, when you put the standard deviation formula all together, i.e., with the square root, you're taking a special kind of distance. This is called the _Euclidean distance_, which is what we use to determine how far away two objects are by a straight line.\n",
        "\n",
        "#### 3.1.4.1 Computing Euclidean distance\n",
        "Supposing we have two lists of numbers: $x = [x_1, \\cdots, x_n]$ and $y = [y_1, \\cdots, y_n]$, the Euclidean distance is:\n",
        "$$d_E(x,y) = \\sqrt{\\sum_{i=1}^n(x_i - y_i)^2}$$\n",
        "\n",
        "This makes the standard deviation the data's distance from a constant list of $\\overline{x}'s$, divided by $\\sqrt{n}$! Let's see how we can take distances using `numpy`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2oQNM729NAh",
        "outputId": "80522ca5-3cf6-494f-dce1-5a85f323e22e"
      },
      "source": [
        "```python\n",
        "## note: y has a NaN, let's filter it out!\n",
        "baseball_data = baseball_data.dropna(axis = \"rows\")\n",
        "\n",
        "## let's make some arrays to take distances\n",
        "x = baseball_data[\"Height\"]\n",
        "y = baseball_data[\"Weight\"]\n",
        "\n",
        "## compute the distanace manually:\n",
        "d = np.sqrt(sum((x - y) ** 2))\n",
        "print(d)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Lgj74K9NAh"
      },
      "source": [
        "#### 3.1.4.2 There's more than one way to compute distance!\n",
        "Notions of distance depend on choices of _norm_, or, how one defines \"big\". In the previous, Euclidean case, big is defined as the square root of the sum of squared values:\n",
        "$$\\|x\\|_2 = \\sqrt{\\sum_{i=1}^nx^2}$$\n",
        "\n",
        "But an alternative, referred to as the _taxicab_ norm:\n",
        "$$\\|x\\|_1 = \\sum_{i=1}^n|x|$$\n",
        "\n",
        "is just the sum of absolute values. This results in the taxicab distance:\n",
        "$$d_T(x,y) = \\sum_{i=1}^n|x_i - y_i|.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn4NOMN79NAh",
        "outputId": "7954f2e0-6d4d-4bc6-be44-b89d47f4cdc8"
      },
      "source": [
        "```python\n",
        "## compute the distnace manually:\n",
        "d = sum(abs(x - y))\n",
        "print(d)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9yrN-669NAh"
      },
      "source": [
        "But why is it called the taxicab distance? Because it is the distance you would travel if you only took right turns to get anywhere, i.e., like on a street grid:\n",
        "\n",
        "<img src=\"https://github.com/anyuanay/DSCI521-summer2025/blob/main/week3/taxicab_distance.png?raw=true\" />\n",
        "\n",
        "There are in fact many more norms and distnaces out there, but our discussion needs to move along into to another concept: association and similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n5Dgh139NAh"
      },
      "source": [
        "#### 3.1.4.3 Exercise: comparing player statistics\n",
        "Represent each player in the baseball data as a (2-dimensional) pair of height/weight numbers. Use either of the Euclidean or taxicab distances on these pairs to determine which players are the 'closest' to one another in terms of build."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am9aPdSD9NAh"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxXgTpAG9NAh"
      },
      "source": [
        "### 3.1.5 Similarity\n",
        "One of the things that we might like to do with a distance measure is compare two lists of data to see how related they are. However, \"far apart\"—which is what distance tells us&mdash;doesn't generally help to figure out if two pieces of data are related (association), or if one distance comparison is a \"closer\" match than another. This is where _similarity measures_ come in, which are like distance metrics, but can produce numbers between $-1$ and $1$, or $0$ and $1$ if they are normalized. Note: a similarity measure outputs $1$ two lists are the same, while a _dissimilarity_ measure outputs $0$.\n",
        "\n",
        "#### 3.1.5.1 Generalizable, norm-based similarity\n",
        "An easy way to produce a normalized similarity from distance is to use the triangle inequality:\n",
        "\n",
        "$$d(x,y) = \\|x - y\\| \\leq \\|x\\| + \\|y\\|$$\n",
        "\n",
        "to get:\n",
        "\n",
        "$$sim(x,y) = \\frac{d(x,y)}{\\|x\\| + \\|y\\|}$$\n",
        "\n",
        "Note that any of these normalized similarity measures have high numbers indicating coming from greater distances. Thus they are actually all _dissimilarity_ measures. Let's try this out with our Euclidean distance:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhl4-tlL9NAh",
        "outputId": "93d6fa60-5e5d-4e34-eed5-fc9f3433314a"
      },
      "source": [
        "```python\n",
        "## compute usual distanace manually:\n",
        "d = np.sqrt(sum((x - y) ** 2))\n",
        "print(d, '\\n')\n",
        "\n",
        "## now compute the similarity manually:\n",
        "sim = np.sqrt(sum((x - y) ** 2)) / (np.sqrt(sum(x**2)) + np.sqrt(sum(y ** 2)))\n",
        "print(sim)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzRMKYg69NAh"
      },
      "source": [
        "#### 3.1.5.2 Similarity for categorical data\n",
        "This is done with sets, using the intersection and union operations. Specifically, the size of an intersection of a pair of sets divided by the size of the pairs union is called the _Jaccard similarity_. Here, higher numbers indicate the sets have more in common, making it a true similarity measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-V28IaE9NAh",
        "outputId": "055078cd-06e3-4653-fbe7-39298a128552"
      },
      "source": [
        "```python\n",
        "## First make two sets\n",
        "set_1 = {\"this\", \"that\", \"and\", \"the\", \"other\"}\n",
        "set_2 = {\"just\", \"that\", \"and\", \"the\", \"other\"}\n",
        "\n",
        "## compute the Jaccard similarity:\n",
        "print(len(set_1.intersection(set_2)) / len(set_1.union(set_2)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjbKsuJa9NAh"
      },
      "source": [
        "#### 3.1.5.3 Cosine similarity\n",
        "One common example is called the cosine similarity:\n",
        "$$ sim(x,y) = \\frac{\\sum_{i=1}^n{x_iy_i}} { \\sqrt{\\sum_{i=1}^nx_i^2} \\sqrt{\\sum_{i=1}^ny_i^2} }, $$\n",
        "\n",
        "which is all about angles and inner products. In fact, it gets you a generalization of the angle between two lines. This means that smaller angles are less similar, making it a true similarity measure. Here's an example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktUbcS3Q9NAh",
        "outputId": "7a6888fd-07a2-4e8a-8dc7-87afada36139"
      },
      "source": [
        "```python\n",
        "## compute the similarity manually:\n",
        "sim = sum(x * y) / (np.sqrt(sum(x ** 2)) * np.sqrt(sum(y ** 2)))\n",
        "print(sim)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z8FvBkf9NAh"
      },
      "source": [
        "### 3.1.6 Association\n",
        "Similarity is still different from association! Instead of similarity, which measures relative closeness, _association_ measures determine if two sets of data rise and fall together. A very common association technique is called _correlation_, of which there are several varieties.\n",
        "\n",
        "#### 3.1.6.1 Linear association\n",
        "The most common correlation measure is probably the Pearson (linear) correlation, which says how close the relationship between $x$ and $y$ is to being a line. As it turns out, Pearson's correlation is closely related to both the Euclidean distance and the cosine similarity:\n",
        "\n",
        "$$ cor(x,y) = \\frac{\\sum_{i=1}^n{(x_i-\\overline{x})(y_i-\\overline{y})}} { \\sqrt{\\sum_{i=1}^n(x_i-\\overline{x})^2} \\sqrt{\\sum_{i=1}^n(y_i-\\overline{y})^2} }. $$\n",
        "\n",
        "In fact, it's just equal to the cosine similarity of data that are standardized (see __Chapter 2__)! Here's a great graphic showing how Pearson's correlation will behave differently for different relationships:\n",
        "\n",
        "<img src=\"https://github.com/anyuanay/DSCI521-summer2025/blob/main/week3/Correlation_examples.png?raw=true\" />\n",
        "\n",
        "Even though Pearson's correlation could be computed from the cosine similarity computation we've already seen, we can just use `SciPy`'s built in to get there:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeKHUIWw9NAh",
        "outputId": "ccd01da5-e950-442a-cbcf-1f034e53f819"
      },
      "source": [
        "```python\n",
        "## compute the pearson correlation\n",
        "## Note the second output is a p-value;\n",
        "## we'll talk about this in a few classes\n",
        "print(scipy.stats.pearsonr(x, y))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRc4lkTh9NAi"
      },
      "source": [
        "#### 3.1.6.2 Monotonic association\n",
        "What if we don't care about _linear_ relationships, but more generally ones for which $y$ either increases or decreases with $x$? One of the biggest assumptions with Pearson's correlation is linearity. This is a pretty strict requirement, and a different type of correlation doesn't care what kind of relationship two pieces of data have, just if they increase/decrease with each other. This is called the Spearman rank correlation, and is computed the same way as Pearson's correlation, but using _ranks_ of $x$ values in place of $x$ values. Computation is quite similar to Pearson:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trNijI1I9NAi",
        "outputId": "2ef78c76-900c-467b-9ae8-5956ef78cd23"
      },
      "source": [
        "```python\n",
        "import scipy.stats\n",
        "## compute the spearman correlation\n",
        "## Note the second output is a p-value;\n",
        "## we'll talk about this in a few classes\n",
        "print(scipy.stats.spearmanr(x, y))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4idtV5X9NAi"
      },
      "source": [
        "Here's a good conceptualization of the difference in how these correlations behave:\n",
        "\n",
        "<img src=\"https://github.com/anyuanay/DSCI521-summer2025/blob/main/week3/Spearman_Pearson.png?raw=true\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQEfA4s29NAi"
      },
      "source": [
        "### 3.1.7 Outliers\n",
        "Sometimes, a few (hopefully) of the numbers in your data might have gotten messed up and not belong. These numbers might have arisen from instrument error, or who knows what. So, when it is okay throw out a number from a data set? In general, this concept and such a number is referred to as an _outlier_. While there is no definite, accepted way to define outliers in a data set, rules of thumb are commonly set according to measure of centrality and spread. Here's two ways to measure outliers:\n",
        "\n",
        "+ All points 1.5 IQRs below the first or below the third quartile\n",
        "+ All points 3 standard deviations ($\\sigma$s) away from the mean\n",
        "\n",
        "#### 3.1.7.1 Computing IQR outliers\n",
        "To perform this kind of outlier computation we have to 1) determine the IQR and its size, and 2) filter values according to a scaling of this region. As it turns out, dataframe objects and boolean masks are very convenient here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG47JHH79NAi",
        "outputId": "c8cccc3b-0416-4be3-c9a5-dd0fbcac1a03"
      },
      "source": [
        "```python\n",
        "## Let's look for 1.5 IQR outliers!\n",
        "## first, computer the IQR\n",
        "IQR = np.percentile(baseball_data[\"Height\"], [25, 75])\n",
        "print(IQR, '\\n')\n",
        "\n",
        "## next, figure out how big the IQR is\n",
        "IQR_size = IQR[1] - IQR[0]\n",
        "print(IQR_size, '\\n')\n",
        "\n",
        "## filter the heights by a boolean mask to get outliers\n",
        "low_outliers = baseball_data[\n",
        "    baseball_data[\"Height\"] < IQR[0] - IQR_size * 1.5\n",
        "][\"Height\"]\n",
        "\n",
        "high_outliers = baseball_data[\n",
        "    baseball_data[\"Height\"] > IQR[1] + IQR_size * 1.5\n",
        "][\"Height\"]\n",
        "\n",
        "print(low_outliers, '\\n')\n",
        "\n",
        "print(high_outliers)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USPPJyoc9NAi"
      },
      "source": [
        "#### 3.1.7.2 Exercise: computing standard deviation-based outliers\n",
        "Modify the above code to implement the outlier detection that defines outliers by as all points at least $3$ standard deviations ($\\sigma$s) away from the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_rPVVmq9NAi"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diePUTqo9NAi"
      },
      "source": [
        "## 3.2 Common visualization methods\n",
        "We've gone through all of these materials on summarization first because many of these actually appear as concepts and quantities that underpin what gets represented in visualization. This will be very apparent in our first type of plot, which is hugely emblematic of EDA: the box and whiskers plot. However, before we begin, it's important to note that just as with summarization, visualization builds up from one variable/dimension to multiples in comparison.\n",
        "\n",
        "### 3.2.0 Getting started with `matplotlib`\n",
        "Before we can actually begin with making visualizations, we'll take a moment to discuss Python's primary visualization functionality. The module for this is called `matplotlib`. However, `matplotlib` visualization don't look that great straight out of the box. In these notes, I'll be sharing some of the tips and tricks I've developed for making `matplotlib` figures look decent, but there are certainly other ways to go about this! For example, there are many hyper-specific and refined visualization tools specially built into `pandas` (though still `matplotlib`, underneath). I won't be going through these, but the basics described herein could get you there with a bit of work.\n",
        "\n",
        "#### 3.2.0.1 One more note\n",
        "Part of the magic of Jupyter notebooks is their capacity to render matplotlib visualizations alongside code and markdown. This does not happen natively, so if you want your visualizations to show up inside of your notebooks, first execute the Jupyter \"magic\" command:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOFIG0p69NAi"
      },
      "source": [
        "```python\n",
        "%matplotlib inline\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yDvZj5J9NAi"
      },
      "source": [
        "Also, don't forget to import matplotlib's pyplot!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hY9MZQL9NAi"
      },
      "source": [
        "```python\n",
        "from matplotlib import pyplot as plt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRAHii879NAi"
      },
      "source": [
        "### 3.2.1 Box and Whiskers Plots\n",
        "Box and whiskers are emblematic of EDA, because they were first proposed by the statistican John Tukey, who promoted EDA early on! Moreover, they are an encapsulation of several descriptive summary statistics for numerical data:\n",
        "\n",
        "+ centrality: through the IQR (the box) and median (central box-line)\n",
        "+ spread: via whiskers, commonly extending out to farthest points within 1.5 IQR-lengths past the limits of the box (first and third quartiles)\n",
        "+ outliers: extreme points past the whiskers are shown as outliers points\n",
        "\n",
        "Note: other box plots might use different standards for the limits of whiskers and their resulting outlier points. It's all up to how you execute the visualization, and `matplotlib` provides control for this. In general, for the down and dirty specifics of `matplotlib` visualizations, consult the docs:\n",
        "\n",
        "+ https://matplotlib.org/api/pyplot_api.html\n",
        "\n",
        "To make our bar plot we can just use the `plt.boxplot` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsmzCh6Z9NAi",
        "outputId": "2332f95d-df89-45ee-f3a8-bdd83bca4338"
      },
      "source": [
        "```python\n",
        "## make a box plot of one data dimension\n",
        "_ = plt.boxplot(baseball_data[\"Height\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2pKY1sp9NAi"
      },
      "source": [
        "#### 3.2.1.2 Making things look nice\n",
        "Don't you think this picture's a bit... ugly? At the very least let's add a label on the x-axis, control the figure dimensions, and add a title to make things a bit more interpretable:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db5iKy3q9NAj",
        "outputId": "df43ee68-f3de-4a2a-e4f4-71ed4b6b7145"
      },
      "source": [
        "```python\n",
        "## Initialize a 6 x 6in figure\n",
        "fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## Make the box plot\n",
        "_ = plt.boxplot(baseball_data[\"Height\"], labels = [\"Height (in)\"])\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Baseball player heights\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V60s3F6f9NAj"
      },
      "source": [
        "But there's usually more optimization to do&mdash;the title, axis ticks and labels are all a little small!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LDxvIPu9NAj",
        "outputId": "55f20d01-e3eb-4b02-9e66-d62515455c31"
      },
      "source": [
        "```python\n",
        "## Initialize a 6 x 6in figure\n",
        "fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## Make the box plot\n",
        "_ = plt.boxplot(\n",
        "    baseball_data[\"Height\"],\n",
        "    labels = [\"Height (in)\"]\n",
        ")\n",
        "\n",
        "## adjust the tick and label font size\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Baseball player heights\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vw0_m169NAj"
      },
      "source": [
        "#### 3.2.1.3 Side by side box and whiskers\n",
        "It's often helpful to see box plots next to one another. This can afford a nice distributional comparison and is likewise made convenient by `pyplot`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tVM6YAW9NAj",
        "outputId": "1ef9e4f0-2a15-4a68-d791-df714d96684f"
      },
      "source": [
        "```python\n",
        "## Initialize a 6 x 6in figure\n",
        "fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## Make the box plot\n",
        "_ = plt.boxplot(\n",
        "    [\n",
        "        baseball_data[\"Height\"],\n",
        "        baseball_data[\"Weight\"],\n",
        "        baseball_data[\"Age\"]\n",
        "    ],\n",
        "    labels = [\n",
        "        \"Height (in)\",\n",
        "        \"Weight (lb)\",\n",
        "        \"Age (yr)\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "## adjust the tick and label font size\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Baseball player heights, weights and ages\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOTxIdYy9NAj"
      },
      "source": [
        "#### 3.2.1.4 Exercise: standardizing data for comparison\n",
        "Use the standardization technique from __Section 2.0.1__ to transform the height, weight, and age columns into numerically comparable quantities and vizualize them in a side-by-side boxplot. How do they appear to differ, now?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okga1RiM9NAj"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiVnZ9sv9NAj"
      },
      "source": [
        "### 3.2.2 Bar Charts\n",
        "Bar plots represent discrete quantities or quantities of categories next to one another. To explore, let's first count up the number of different numbers of players in each position type, and display them in a barplot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h63B_z069NAj"
      },
      "source": [
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "## initialize a counter for the positions\n",
        "positions = Counter()\n",
        "\n",
        "## loop over rows to count up the positions\n",
        "for ix, row in baseball_data.iterrows():\n",
        "    positions[row[\"Position\"]] += 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ovshKdU9NAj",
        "outputId": "ebd4d62c-8732-424f-d375-dbdb5cc85277"
      },
      "source": [
        "```python\n",
        "list(positions.keys())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lHKoTiy9NAj"
      },
      "source": [
        "After a little pre-processing we're ready to plot using `plt.bar()`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtWBeS0m9NAj",
        "outputId": "cf3cb623-5b4a-482b-bb1c-d7e3e2d2e4fe"
      },
      "source": [
        "```python\n",
        "## It's width by height, so let's make this one wide!\n",
        "bar_fig = plt.figure(figsize = (12, 6))\n",
        "\n",
        "## note the plot.bar() function takes an x-positon for bars\n",
        "## in the event that data are not categorical,\n",
        "## but we'll just use discrete positioning, i.e., left = [1,2,3, ...]\n",
        "## to space out the bars\n",
        "_  = plt.bar(\n",
        "    x = range(1, len(positions) + 1),\n",
        "    height = list(positions.values()),\n",
        "    tick_label = list(positions.keys())\n",
        ")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Baseball player positions\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Number players\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCR299aP9NAj"
      },
      "source": [
        "Again, it's important to make things look nice. Since the labels are colliding, let's rotate them and replace the underscores with newlines!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGJgC_oE9NAj",
        "outputId": "98beca57-846f-4397-8c2f-f24acb6c6d91"
      },
      "source": [
        "```python\n",
        "## we need regular expressions to fix the labels!\n",
        "import re\n",
        "\n",
        "## a little more pre-processing to fix the labels\n",
        "labels = positions.keys()\n",
        "labels = [\n",
        "    re.sub(\"_\", \"\\n\",label)\n",
        "    for label in labels\n",
        "]\n",
        "\n",
        "## It's width by height, so let's make this one wide!\n",
        "bar_fig = plt.figure(figsize = (12, 6))\n",
        "\n",
        "## note the plot.bar() function takes an x-positon for bars\n",
        "## in the event that data are not categorical,\n",
        "## but we'll just use discrete positioning, i.e., left = [1,2,3, ...]\n",
        "## to space out the bars\n",
        "_  = plt.bar(\n",
        "    x = range(1, len(positions) + 1),\n",
        "    height = list(positions.values()),\n",
        "    tick_label = labels\n",
        ")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Baseball player positions\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Number players\", fontsize = 15)\n",
        "\n",
        "## rotate the x-tick labels\n",
        "_ = plt.xticks(rotation = 45)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8XKaYpH9NAk"
      },
      "source": [
        "#### 3.2.2.1 Aside: what about pie charts?\n",
        "While it's true, pie charts represent perhaps even more information as a categorical bar plot (wholeness) they are challenged by inerpretability. Rather than displaying a comparison of quantities via heights, pie charts rely on potentially two quantitative concepts that challenge human intuition:\n",
        "\n",
        "+ angle: the breadth of the wedge\n",
        "+ area: the overall size of the wedge\n",
        "\n",
        "While either of a wedge's angle or area is sufficient to represent differences in quantity, these representations and significantly more complex than bar height, which can be compared by simply placing a ruler or drawing a horizontal line. When you want to compare quantities use bar charts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrByr6Xj9NAk"
      },
      "source": [
        "#### 3.2.2.2 Exercise: a sorted bar plot\n",
        "Utilize the `sorted()` function from __Sec. 3.1.1__ to rebuild the barplot visualization of baseball position, but with the size of bars decreasing from left to right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J76kfCSD9NAk"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeLrQq6X9NAk"
      },
      "source": [
        "### 3.2.3 Histograms\n",
        "While bar plots are very general, allowing the user to control the exact heights and positions of bars, histograms a more specific kind of bar plot intended to represent density in 1-dimensional data. In other words, histograms provide information on centrality and variation, without directing the eye to a single quantity, like mean, median, or standard deviation. However, these values can be plotted on top.\n",
        "\n",
        "#### 3.2.3.1 A caveat: binning\n",
        "It's important to note that in order to represent density, histograms must collect data into bins. The question is:\n",
        "\n",
        "+ how are the bin boundaries computed?\n",
        "\n",
        "One common methods is:\n",
        "\n",
        "+ a fixed number of equally-spaced bins\n",
        "\n",
        "However, the number of bins and their spacing are parameters that have to be set. This can be done with the `matplotlib` `hist` function, but there are defaults.\n",
        "\n",
        "Let's make a histogram of the heights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hMvUNdn9NAk",
        "outputId": "3c7ee6d4-3f88-418c-f3e4-311e441f5de7"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "hist_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make a histogram and set the number of bins to 15\n",
        "_  = plt.hist(baseball_data[\"Height\"], bins = 15)\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Baseball player heights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Number players\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Height (in)\", fontsize = 15)\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pcf_8Tz9NAk"
      },
      "source": [
        "#### 3.2.3.2 Exercise: binning\n",
        "Repeat the above visualization using a few different bin sizes. Which choice appears to evoke the smoothest shape in the hisogram? What happens when there are too many or too few bins?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqiEnIiz9NAk"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3NWoOc59NAk"
      },
      "source": [
        "#### 3.2.3.3 Normalized histograms\n",
        "\n",
        "We'll be discussing normalization a bit more when we discuss probability, but here it means we're creating some sort of a probabilistic representation&mdash;for now, a collection of non-negative numbers that 'adds up' to 1. However, a normalized histogram's bar heights do not add up to one!\n",
        "\n",
        "This is a common misconception. Distribution normalization is all about _area_. So it's actually the bar areas that add up to one. In other words, you would have to sum the products of the widths and heights. So, don't be surprised when you see a normalized histogram whose bar heights go above 1; this is just because the bars are very skinny!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7rn4GQ79NAk",
        "outputId": "c47f9c0c-70ab-4ff3-dac0-eeace8c58a02"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "hist_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make a histogram and set the number of bins to 15\n",
        "_  = plt.hist(baseball_data[\"Height\"], bins = 15, density=True)\n",
        "\n",
        "# The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg in the above function\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Baseball player heights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Number players\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Height (in)\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKJYHv0y9NAk"
      },
      "source": [
        "#### 3.2.3.4 Comparing histograms\n",
        "\n",
        "What if we want to compare histogram of the baseball player heights and weights? Well, heights and weights are pretty different numbers, so the first thing we'll have to do is standardize out the data (as in __Sec. 2.0.1__). Once we do this, we can make two histograms that are different colors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZSnR3iS9NAk"
      },
      "source": [
        "```python\n",
        "## pre-processing: standardize the heights and weights\n",
        "s_height = baseball_data[\"Height\"] - np.mean(baseball_data[\"Height\"])\n",
        "s_height = s_height / np.std(baseball_data[\"Height\"])\n",
        "\n",
        "s_weight = baseball_data[\"Weight\"] - np.mean(baseball_data[\"Weight\"])\n",
        "s_weight = s_weight / np.std(baseball_data[\"Weight\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0UMa7-n9NAk",
        "outputId": "0384d392-966e-4f20-911b-82291cf40563"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "hist_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make two histogram that ar different colors\n",
        "_  = plt.hist(s_height, color = \"blue\")\n",
        "_  = plt.hist(s_weight, color = \"red\")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Standardized player heights and weights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Number players\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Standardized values\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4dwmh7N9NAk"
      },
      "source": [
        "Note that since the colors are 'solid' it's hard to see what's going on where they overlap.\n",
        "\n",
        "Well, images&mdash;at least, pngs&mdash;have an extra byte devoted to transparency in each pixel. We can use transparency in a histogram to display overlap, too. Right now, this would make our overlapping region purple. However it's a little tricky for some people to see color. Thus, we might want to use shades of gray vs red, just to be safe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bO6HX1l9NAl",
        "outputId": "486acb5f-3bc1-4612-c501-ec9009246b39"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "hist_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make two histogram that are different colors\n",
        "## and both have alpha = 0.25 (transparency)\n",
        "_  = plt.hist(s_height, color = \"black\", alpha = 0.25)\n",
        "_  = plt.hist(s_weight, color = \"red\", alpha = 0.25)\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Standardized player heights and weights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Number players\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Standardized values\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cWg8woU9NAl"
      },
      "source": [
        "#### 3.2.3.5 Adding a legend\n",
        "But wait, which histogram is which??? Right, even though you can see the code, the reader doesn't know what red vs. black indicates! So, it looks like we're going to need a legend! Legends are a little tricky in Python. You have to put a label in each separate visualization command, and then call the legend command afterwords. But it's much easier to read!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6LP4pYy9NAl",
        "outputId": "be8e089c-5aeb-4bc2-e5c3-d678d88e1dbf"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "hist_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make two histogram that are different colors\n",
        "## and both have alpha = 0.25 (transparency)\n",
        "_  = plt.hist(s_height, color = \"black\", alpha = 0.25, label = \"Height\")\n",
        "_  = plt.hist(s_weight, color = \"red\", alpha = 0.25, label = \"Weight\")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Standardized player heights and weights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Number players\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Standardized values\", fontsize = 15)\n",
        "\n",
        "## make the legend\n",
        "_ = plt.legend(fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l68NoxC-9NAl"
      },
      "source": [
        "### 3.2.4 Scatter Plots\n",
        "Okay, so we're starting to make visual comparisons of our data. It looks like the heights might be a bit more spread out than the weights from our histograms, but we might want to know if heights and weights are related to each other, player-by-player. This is where a scatter, or point plot comes in handy. This is now, offically, a two-dimensional plot! Note: for our scatter plot it won't matter if we use standardized data! So, let's just use the data in original units for interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq4JeKBR9NAl",
        "outputId": "849e22df-d6ba-4dc1-f350-b4be1e590210"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "scat_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make a scatter plot\n",
        "_  = plt.scatter(\n",
        "    baseball_data[\"Height\"],baseball_data[\"Weight\"],\n",
        "    color = \"black\"\n",
        ")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Player heights vs. weights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Weight (lb)\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Height (in)\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdX3fCO9NAl"
      },
      "source": [
        "#### 3.2.4.1 Density\n",
        "Scatter plot interpretation is all about density. Above, the dots are mostly on top of one another. Two things we can easily do are:\n",
        "\n",
        "1. make the dots smaller\n",
        "2. add 'alpha' (transparency) to convey density"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Qnvt_D9NAl",
        "outputId": "047aaec6-ff1e-41fe-dddf-062335125b33"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "scat_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make a scatter plot\n",
        "_  = plt.scatter(\n",
        "    baseball_data[\"Height\"],baseball_data[\"Weight\"],\n",
        "    color = \"black\", s = 5, alpha = 0.5\n",
        ")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Player heights vs. weights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Weight (lb)\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Height (in)\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ro4S3eL9NAl"
      },
      "source": [
        "#### 3.2.4.1 2-dimensional binning: an alternative to scatter plot density\n",
        "Unlike histograms, which use bar height to represent density, two dimensions require color to represent density. However, just like histograms there has to be some kind of binning. An interesting way to do this is with a hexagonal binning. It's pretty much as straightforward as a histogram, but this means that we need to be conscious of the number of bins (gridsize) we use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X5zmvZr9NAl",
        "outputId": "aaf31b98-5cb1-4d34-f36f-3d0edf42bfc4"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "hexb_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make a hexbin plot\n",
        "_  = plt.hexbin(\n",
        "    baseball_data[\"Height\"],baseball_data[\"Weight\"], color = \"black\",\n",
        "    gridsize = 25\n",
        ")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Player heights vs. weights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Weight (lb)\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Height (in)\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj5KryCi9NAl"
      },
      "source": [
        "#### 3.2.4.1 Color maps\n",
        "The default _color map_, or, scale of colors used, isn't so easy to understand, especially for the color-challenged. Here's a some discussion of maps:\n",
        "\n",
        "+ https://matplotlib.org/users/colormaps.html\n",
        "\n",
        "Let's just use black-to-white for simplicity:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAFuK2Y09NAl",
        "outputId": "ba37fe03-29e0-4ca1-9aa4-f673063881e8"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "hexb_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## make a hexbin plot\n",
        "_  = plt.hexbin(\n",
        "    baseball_data[\"Height\"],baseball_data[\"Weight\"], color = \"black\",\n",
        "    gridsize = 25, cmap = \"binary\"\n",
        ")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Player heights vs. weights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Weight (lb)\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Height (in)\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTXQSIb59NAl"
      },
      "source": [
        "#### 3.2.4.2 Exhibiting a relationship\n",
        "So, do height and weight have a relationship? Well, maybe, but it's fuzzy. Just how fuzzy is something we've already been thinking about. We found some correlations—Pearson, Spearman—between height and weight, and a legend is actually a great place to display them. Let's see how we can get some more interesting information into a legend:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV97Zgo09NAl",
        "outputId": "9f38aed6-e9b7-4302-c628-e90924c4853a"
      },
      "source": [
        "```python\n",
        "## set the figure dimensions\n",
        "hexb_fig = plt.figure(figsize = (6, 6))\n",
        "\n",
        "## To put a (Pearson) correlation into a figure\n",
        "## we must process the relationship between the two\n",
        "## variables and save the output as a string:\n",
        "\n",
        "cor = scipy.stats.pearsonr(\n",
        "    baseball_data[\"Height\"],baseball_data[\"Weight\"]\n",
        ")[0]\n",
        "\n",
        "## for clarity, it's probably best to round\n",
        "## to a few decimal places, first\n",
        "## but we should also label it as \"r\"\n",
        "corstr = \"r = \" + str(round(cor, 2))\n",
        "\n",
        "## make a hexbin plot\n",
        "_  = plt.hexbin(\n",
        "    baseball_data[\"Height\"],baseball_data[\"Weight\"], color = \"black\",\n",
        "    gridsize = 25, cmap = \"binary\", label = corstr\n",
        ")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"Player heights vs. weights\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Weight (lb)\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.xlabel(\"Height (in)\", fontsize = 15)\n",
        "\n",
        "## make a legend\n",
        "_ = plt.legend(fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXGbagr9NAl"
      },
      "source": [
        "### 3.2.5 Line Plots\n",
        "Line plots require two dimensions of data, just like scatter plots, but there is one major difference:\n",
        "\n",
        "+ the x-variable of a line plot must have an inherent order\n",
        "\n",
        "So, while our baseball player's heights don't have a necessary meaningful order, _time_ does. Line plots for which time is the x-variable are referred to as _time series_. Let's look at some Apple stock data and see if we can plot the price changes as a time series. Note that if we want the dates to truly be numeric type objects that we can plot in order on the x-axis, they must be parsed as dates!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbFw_kz29NAm",
        "outputId": "eec8b6d7-577b-4ac6-af8e-a6bf4b3fdda2"
      },
      "source": [
        "```python\n",
        "## This loads the csv file from disk\n",
        "APPL = pd.read_csv(\n",
        "    filepath_or_buffer = \"APPL.csv\", sep = \",\",\n",
        "    header=0, parse_dates = [0]\n",
        ")\n",
        "\n",
        "## set the figure dimensions\n",
        "line_fig = plt.figure(figsize = (12, 6))\n",
        "\n",
        "## make a line plot\n",
        "_  = plt.plot(APPL[\"Date\"],APPL[\"Close\"], color = \"black\")\n",
        "\n",
        "## Set the tick and label fontsize\n",
        "plt.tick_params(labelsize = 15)\n",
        "\n",
        "## Set the title\n",
        "_ = plt.title(\"APPL closing prices\", fontsize = 15)\n",
        "\n",
        "## Set the y-label\n",
        "_ = plt.ylabel(\"Closing price ($$$)\", fontsize = 15)\n",
        "\n",
        "## Set the x-label\n",
        "_ = plt.xlabel(\"Date\", fontsize = 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swx_jA239NAm"
      },
      "source": [
        "#### 3.2.5.1 Exercise: line charts\n",
        "Make a line chartof the Apple stock above, except this time plot the daily price change as a a function of date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shSQUDYT9NAm"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noENrlH39NAm"
      },
      "source": [
        "#### 3.2.4.3 Exercise: combining visual elements\n",
        "While it won't change the above correlation in any way, rebuild the density plot visualization from __Sec. 3.2.4.2__ using standardized height and weight values. Then, add to this visualization by plotting the line $y = x$. Discuss the stength of correlation and how drastically the density appears to fall out of alignment with this most basic linear relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB5ybrPz9NAm"
      },
      "source": [
        "```python\n",
        "## code here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkXOAoR_9NAm"
      },
      "source": [
        "#### 3.2.7 Interactivity\n",
        "So far we've gone through making a number of the basic visualizations. These are techniques that have all been out there for a while, and the big deal with data science now is the ease with which they can be produced. But coming back to what the point of visualization and summarization are:\n",
        "\n",
        "+ intuitive communication and a big picture view.\n",
        "\n",
        "These basic visualization techniques are really just the tip of the visualization iceberg. There are a ton of other highly-specialized ways to visualize information now, but on their own __these are all still static views of data__.\n",
        "\n",
        "While a static view might allow you to inspect all of the data in a single or few dimensions, there's a problem: data are now big. Big meaning a lot, but perhaps more importantly here, _varietal_. Instead of just height or weight, etc., a data set like Project Gutenberg might enable you to build one visualization for each of 50,000 books! How would you represent all of these 50,000 visualization? How would you ever bother to make them all and would you want them to persist?\n",
        "\n",
        "The answer to all of these questions is \"no\", make your visualizations _interactive_. Interactive visualization might mean having a dropdown menu or search bar to be able search through all 50,000 books to be able to temporarily generate the book-visualization of interest. Generally, interactive visualizations are web-based, and Python is not the best for web programming. However, it does have a library for this called `Bokeh`:\n",
        "\n",
        "+ The docs: https://bokeh.pydata.org/en/latest/\n",
        "+ The gallery: https://bokeh.pydata.org/en/latest/docs/gallery.html\n",
        "\n",
        "We're not going to go through `Bokeh` here, because interactive visualization is really an in-depth topic unto itself. Moreover, the way Bokeh works is as an API for a more-natively web-based language: Javascript. Thus, the most established and native interactive visualization toolkit is actually a Javascript library called `d3.js`:\n",
        "\n",
        "+ information: https://d3js.org/\n",
        "+ gallery: https://github.com/d3/d3/wiki/Gallery\n",
        "\n",
        "While `d3.js` is a Javascript library, perhaps the most fun examples to look at were produced by its creator, Mike Bostock, through his bl.ocks.org GitHub gist:\n",
        "\n",
        "+ bl.ocks: https://bl.ocks.org/-/about/\n",
        "+ Bostock's bl.ocks: https://bl.ocks.org/mbostock\n",
        "\n",
        "Note: Mike Bostock was the graphics editor for The New York Times for years. He had a massive impact on the world of information visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ljsuxd9NAm"
      },
      "source": [
        "```python\n",
        "\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}