{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/DSCI521-summer2025/blob/main/week3/DSCI521_week3_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiG29uQHDSRn"
      },
      "source": [
        "# DSCI 521: Methods for analysis and interpretation <br> Chapter 3: Exploratory data analysis and visualization\n",
        "\n",
        "## Exercises\n",
        "Note: numberings refer to the main notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a72Dwvc-DSRp"
      },
      "source": [
        "#### 3.1.1.2 Exercise: modifying a sort order\n",
        "Use the `key` argument and a lambda function to sort the `list_of_tuples` object primarily by the second (string) column, and secondarily by the first (integer) column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UbykadJDSRp"
      },
      "outputs": [],
      "source": [
        "list_of_tuples = [\n",
        "    (8, \"g\"), (6, \"a\"), (7, \"f\"), (5, \"d\"),\n",
        "    (3, \"k\"), (0, \"t\"), (9, \"x\"), (8,  \"f\")\n",
        "]\n",
        "\n",
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W3eVycyDSRq"
      },
      "source": [
        "#### 3.1.3.2 Exercise: percentiles\n",
        "Compute all deciles ($10$-increment percentiles) for the baseball player heights. How do these values space apart, are the locations evenly spaced?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVQtOiXWDSRq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# import scipy.stats\n",
        "import pandas as pd\n",
        "\n",
        "baseball_data = pd.read_csv(\n",
        "    filepath_or_buffer= \"data/baseball_heightweight.csv\",\n",
        "    sep = \",\",\n",
        "    header = 0\n",
        ")\n",
        "\n",
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Z6RSDfDSRq"
      },
      "source": [
        "#### 3.1.4.3 Exercise: comparing player statistics\n",
        "Represent each player in the baseball data as a (2-dimensional) pair of height/weight numbers. Use either of the Euclidean or taxicab distances on these pairs to determine which players are the 'closest' to one another in terms of build."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzNUbuaPDSRr"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz8S37PCDSRr"
      },
      "source": [
        "#### 3.1.7.2 Exercise: computing standard deviation-based outliers\n",
        "Modify the above code to implement the outlier detection that defines outliers by as all points at least $3$ standard deviations ($\\sigma$s) away from the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZkmJqWJDSRr"
      },
      "outputs": [],
      "source": [
        "height_stdev = np.std(baseball_data[\"Height\"])\n",
        "height_mean = np.mean(baseball_data[\"Height\"])\n",
        "\n",
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K7StlPrDSRr"
      },
      "source": [
        "#### 3.2.1.4 Exercise: standardizing data for comparison\n",
        "Use the standardization technique from __Section 2.0.1__ to transform the height, weight, and age columns into numerically comparable quantities and vizualize them in a side-by-side boxplot. How do they appear to differ, now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHgW0hvSDSRs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "def standardize(data):\n",
        "    mean = np.mean(data)\n",
        "    stdev = np.std(data)\n",
        "\n",
        "    standardized_data = (data - mean) / stdev\n",
        "\n",
        "    return standardized_data\n",
        "\n",
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkXTIY0fDSRs"
      },
      "source": [
        "#### 3.2.2.2 Exercise: a sorted bar plot\n",
        "Utilize the `sorted()` function from __Sec. 3.1.1__ to rebuild the barplot visualization of baseball position, but with the size of bars decreasing from left to right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYIunXtzDSRs"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "## initialize a counter for the positions\n",
        "positions = Counter()\n",
        "\n",
        "## loop over rows to count up the positions\n",
        "for ix, row in baseball_data.iterrows():\n",
        "    positions[row[\"Position\"]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNPVXbXoDSRs"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-tmfuv2DSRs"
      },
      "source": [
        "#### 3.2.3.2 Exercise: binning\n",
        "Repeat the above visualization using a few different bin sizes. Which choice appears to evoke the smoothest shape in the hisogram? What happens when there are too many or too few bins?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwRP8nUODSRs"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-pUvnybDSRs"
      },
      "source": [
        "#### 3.2.5.1 Exercise: line charts\n",
        "Make a line chartof the Apple stock above, except this time plot the daily price change as a a function of date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BCqWURHDSRs"
      },
      "outputs": [],
      "source": [
        "## This loads the csv file from disk\n",
        "APPL = pd.read_csv(\n",
        "    filepath_or_buffer = \"./data/APPL.csv\", sep = \",\",\n",
        "    header=0, parse_dates = [0]\n",
        ")\n",
        "\n",
        "APPL = APPL.sort_values(by = \"Date\", ascending = True).reset_index(drop = True)\n",
        "\n",
        "APPL[\"Change\"] = APPL[\"Close\"].diff().fillna(0)\n",
        "\n",
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muLpof1TDSRs"
      },
      "source": [
        "#### 3.2.4.3 Exercise: combining visual elements\n",
        "While it won't change the above correlation in any way, rebuild the density plot visualization from __Sec. 3.2.4.2__ using standardized height and weight values. Then, add to this visualization by plotting the line $y = x$. Discuss the stength of correlation and how drastically the density appears to fall out of alignment with this most basic linear relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwybXq7kDSRs"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZA3E6vbDSRt"
      },
      "source": [
        "It looks like there's some variation in weight between players who have the same height. Below the mean height, this variation is mostly above the $y = x$ line, meaning that the players who are of below-average standardized height tend to have above-average standardized weight. On the other hand, players who are of above-average standardized height tend to have below-average standardized weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-1p47ipglpg"
      },
      "source": [
        "## Additional In-depth Exercises\n",
        "\n",
        "### A. Rank-frequency distributions and Zipf's Law\n",
        "\n",
        "Zipf's Law is a loose, common phenomenon that's readily-observable in many forms of social data. Generally, when some collection of categorical entities, e.g., words ($w$) are counted ($f(w)$), their _ranked_ values, i.e., being ordered, $r = 1, \\dots$ from large-to-small by frequency forms a simple, reciprocal, 'power-law' relationship:\n",
        "\n",
        "$$\n",
        "f(w_r) = C\\cdot r^{-1}\n",
        "$$\n",
        "\n",
        "for some constant, $C$. In this set of exercises we'll explore the universality of this relationship between words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxs3fZMKDSRt"
      },
      "source": [
        "#### A.1 Frankenstein rank vs frequency\n",
        "First, plot this rank-frequency relationship for a processed copy of the Frankstein book.\n",
        "In particular, load the `'./data/84-processed.json'` file as `data` and plot the logarithm via `np.log10()`\n",
        "of the `data['rs']` vs the `data['fs']` sets of values as a line plot. Additionally, plot Zipf's Law, scaled\n",
        "so that the lowst frequency in the Zipf model has value 1.\n",
        "\n",
        "When you do this make sure you put a title and legend on the figure,\n",
        "and addition support readability as much as possible,\n",
        "including plot color, font size, line style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I11V3oq3DSRt"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxQtlmcsDSRt"
      },
      "source": [
        "#### A.2 Histogram of Novelty\n",
        "Now make a histogram of a different column, particularly, `data['As']`. This is called the novelty function,\n",
        "which, as a function of word order (let's call this $r$) indicates the probability that a word has not been observed before in the document.\n",
        "Again, when you do this make sure you put a title and legend on the figure,\n",
        "and addition support readability as much as possible,\n",
        "including plot color, font size, line style.\n",
        "\n",
        "Additionally, plot a vertical line for the mean value of novelty in this histogram and comment on the on the scale of variation in the data&mdash;is variation 'wild' or or 'confined' to a limited reggion? Also is the mean a good approximation of centrality?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDjvGNDTDSRt"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gthtn_V0DSRt"
      },
      "source": [
        "#### A.3 Frankenstein novelty scatter plot\n",
        "As it turns out, the novelty function exhibits some relatively-smooth variation with respect to rank. Make a scatter plot of the novelty function and plot the mean novelty function as a horizontal line.\n",
        "\n",
        "As before, when you do this make sure you put a title and legend on the figure,\n",
        "and addition support readability as much as possible,\n",
        "including plot color, font size, line style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gzz-JxLDSRt"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fiTB0EVDSRt"
      },
      "source": [
        "#### A.4 Lowest-loss word2vec optimized novelty\n",
        "Now, unzip the `'./data/84-performance.zip'` file and subsequently load the `'./data/84-performance.json'` file, which exhibits a number of randomly-initialized, gradient-descent optimized `'model'` for the novelty function. This object (name it `performance`) has additional fields named `'NLL'` (the negative log likelihoods, loss values) and `'correlation'` (the association of each model to the known likelihood function).\n",
        "\n",
        "Determine which novelty function has the lowest `'NLL'` value and plot it in the background of the empirical novelty, utilizing any code from the previous part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wntYYGfEDSRt"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvVOLcXQDSRt"
      },
      "source": [
        "#### A.5 Plots of model convergence\n",
        "Now, make two plots by iteration (keys in the `performance` object) of the performance values (`'NLL'` and `'correlation'`)\n",
        "and observe the location of minimum `'NLL'` value as a vertical line. Does it appear to align to the correlation with the\n",
        "known novelty values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMKXG5DKDSRt"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYZ3j84IDSRt"
      },
      "source": [
        "#### A.5 Plot empirical frequency and novelty/word2vec transforms\n",
        "Next, we'll observe the relationship between frequency and novelty. In particular reuse the frequency-plotting from the first part of this exercise, but additionall utilize the\n",
        "`frequency_model` function provided below, which transforms a novelty function into a corresponding frequency model. Use this to compare the empirical frequency values with\n",
        "to 1) Zipf's law, 2) the frequency model implied by the Theoretical novelty function, and 3) the frequency model implied by the Learned novelty function (from the `performance` object).\n",
        "Note, the novelty function used from the `performance` object should be the one which has the smallest negative log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JdLDy-XDSRt"
      },
      "outputs": [],
      "source": [
        "def frequency_model(As, ns):\n",
        "    Am = ns/np.cumsum(1/As)\n",
        "    ## approximate birthdays\n",
        "    mn = (ns + Am - 1)/Am\n",
        "    ## slicing Ashat from second, up, assumes the As are actually one behind\n",
        "    ## these are the log-transformed Eq. 8 factors\n",
        "    NLfactors = list(-(1 - As[1:])*np.log10(mn[:-1]/mn[1:]))\n",
        "    ## this is a really tricky cumulative sum, since it operates in reverse for the factors,\n",
        "    ## to make a cumulative product after the re-exponentiation (removing the log transformation)\n",
        "    return np.floor(10 ** np.append(np.cumsum(NLfactors[::-1])[::-1], 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny7SLhaxDSRt"
      },
      "outputs": [],
      "source": [
        "## code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}